<!doctype html>
<html lang="en">

<head>
    <title>Zhao Zhang</title>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="./css/bootstrap.min.css">
    <style>
        .vspace-top {
            margin-top: 30px;
        }

        .cardhead-bg {
            /* background: url("./img/color.png") no-repeat center; */
            background-color: rgb(255, 255, 255);
            /* background-size: 100% 100%; */
        }
        .jum-shadow{
            -moz-box-shadow:2px 2px 6px #9C9C9C;
            -webkit-box-shadow:2px 2px 6px #9C9C9C;
            box-shadow:2px 2px 6px #9C9C9C;

        }
        .jumbotron{
            background-image: url("./img/page_img/liu_duck1.png");
            /* background-color: rgb(255, 255, 255); */
            background-repeat:no-repeat;
            /* background-size:  15% 95%; */
            /* background-position:bottom right; */
            background-position:70% 100%;
        }
        .vspace-top-left {
            margin-top: 25px;
            margin-left: 25px;
        }

        .vspace-top {
            margin-top: 30px;
        }

        .vspace-top-lite {
            margin-top: 10px;
        }

        .vspace-minitop {
            margin-top: 20px;
        }

        .paper-title {
            margin-top: 8px;
            font-weight: bold;
            color: gray;
            font-size:larger;
        }

        .paper-authors {
            font-style: italic;

        }

        .paper-pub {
            font-weight: bold;
            color: gray;
            font-style: italic
        }
    </style>
</head>

<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
            <a class="navbar-brand" href="index.html">Home</a>
            <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive"
             aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="./publications.html">Publications
                            <span class="sr-only">(current)</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="./data.html">Codes/Datasets
                            <span class="sr-only">(current)</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#">Useful links
                            <span class="sr-only">(current)</span>
                        </a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>


    <div class="jumbotron">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <h1>
                        Zhao Zhang ÔºàÂº† ÈíäÔºâ
                    </h1>
                    <p style="text-align:justify; text-justify:inter-ideograph">
                        I'm currently working as a Vision-Language Researcher at ByteDance,
                        with a focus on multimodal LLMs and their applications.
                        I completed my Master's degree at Nankai University,
                        where I was under the supervision of
                        <a href="https://mmcheng.net/cmm/" style="color:black">Ming-Ming Cheng</a>.
                        <!-- Now, my research interest mainly focuses on . -->
                        Please feel free to contact me at
                        (üìÆ: zzhangü•≥mailüîÖnankaiüîÖeduüîÖcn)
                    </p>
                    <p>
                        <!-- For more information, please consult my 
                        <a href="./me/CV.pdf" target="_black">CV</a>. -->
                        You can also find me in 
                        <a href="https://scholar.google.com/citations?user=Wcj40PMAAAAJ">
                            <img src="./img/page_img/google.png" height="18" width="18"/>
                        </a>
                        and 
                        <a href="https://space.bilibili.com/184678848">
                            <img src="./img/page_img/bilibili-fill.png" height="23" width="25"/>
                        </a>
                        <!-- <a href="https://zhihu.com/people/zao-shui-zao-qi-duan-lian-shen-ti">
                            <img src="./img/page_img/zhihu-line.png" height="22" width="25"/>
                        </a>. -->
                        <!-- <a href="https://www.yuque.com/z_zhang">
                            <img src="./img/page_img/yq.png" height="22" width="25"/>
                        </a>. -->
                    </p>
                </div>
            </div>
        </div>
    </div>

    

    <div class="container">
        <!-- News -->
        <section class="row vspace-top">
            <div class="col-lg-10">
                <h2>Recent News</h2>
                    <ul>
                        
                        <li class="list-item">
                            [üîù/2025] üå≥ We're seeking interns passionate about Graphic Design to collaborate on impactful research projects. Please feel free to contact me via email.
                        </li>
                        <li class="list-item">
                            [06/2025] We released <span style="font-weight: bold;"><a href="https://github.com/graphic-design-ai/creatiposter" target="_black">CreatiPoster</a></span> <img src="./papers/25_creatiposter/creatiposter_logo.png" alt="logo" style="height: 18px; width: auto; vertical-align: middle;" />, an AI-driven graphic design generation system for multi-layer and editable compositions with strong visual appeal.
                        </li>
                        <li class="list-item">
                            [05/2025] Our Unified-MLLM for image layer decomposition was was accepted by ICML 2025. The technical report will be released soon.
                        </li>
                        <li class="list-item">
                            [01/2025] RelationLMM was accepted by TPAMI-25.
                        </li>
                        <li class="list-item">
                            [12.2024] Our <a href="https://arxiv.org/abs/2305.12452" target="_black">GRES</a> is selected as an "Excellent Science & Technology Academic Paper" for the 2024 Shenzhen 4th Excellent Science & Technology Academic Paper Selection.
                        </li>
                        <li class="list-item">
                            [04/2024] üî• <span style="font-weight: bold;"><a href="https://github.com/graphic-design-ai/graphist">Graphist</a></span> <img src="./papers/24_graphist/logo.png" alt="logo" style="height: 22px; width: auto; vertical-align: middle;" /> was accepted by AAAI 2025. We have unleashed the potential of MLLM in graphic design.
                        </li>
                        <li class="list-item">
                            [02/2024] One paper was accepted by CVPR 2024.
                        </li>
                        <li class="list-item">
                            [09/2023] <a href="https://github.com/shikras/d-cube?tab=readme-ov-file">D-Cube</a> was accepted by NeurlPS 2023.
                        </li>
                        <li class="list-item">
                            [08/2023] We released <span style="font-weight: bold;"><a href="https://github.com/isekai-portal/Link-Context-Learning">Link-Context Learning</a></span> for MLLMs as well as an interesting dataset <span style="font-weight: bold;"><a href="https://github.com/isekai-portal/Link-Context-Learning">ISEKAI</a></span>.
                        </li>
                        <li class="list-item">
                            [07/2023] <span style="font-weight: bold;"><a href="https://arxiv.org/abs/2305.12452">GRES</a></span> was accepted by ICCV 2023.
                        </li>
                        <li class="list-item">
                            [06/2023] üî• We released <span style="font-weight: bold;"><a href="https://github.com/shikras/shikra">Shikra</a></span> <img src="./papers/23_shikra/logo.png" alt="logo" style="height: 22px; width: auto; vertical-align: middle;" />, an awesome MLLM for Referential Dialogue.
                        </li>
                        <li class="list-item">
                            [01/2023] One paper was accepted by TPAMI.
                        </li>
                        <li class="list-item">
                            [08/2022] One paper was accepted by ECCV 2022.
                        </li>
                        <li class="list-item">
                            [07/2022] Two papers was accepted by ACM MM 2022.
                        </li>
                        <li class="list-item">
                            [07/2022] I am working as vision-language researcher in SenseTime Research.
                        </li>
                        <li class="list-item">
                            [06/2022] One paper was accepted by CVMJ.
                        </li>
                        <li class="list-item">
                            [03/2022] One paper was accepted by CVPR 2022 as oral presentation.
                        </li>
                        <li class="list-item">
                            [12/2020] I am working as an intern in Tencent Youtu Lab.
                        </li>
                        <li class="list-item">
                            [12/2020] One paper was accepted by TIP 2021.
                        </li>
                        <li class="list-item">
                            [07/2020] One paper was accepted by ECCV 2020.
                        </li>
                        <li class="list-item">
                            [05/2020] One paper was accepted by TNNLS 2020.
                        </li>
                        <li class="list-item">
                            <!-- <span class="text-danger">[Top]</span> One paper was accepted by CVPR 2020 (CCF-A). -->
                            [04/2020] One paper was accepted by CVPR 2020.
                        </li>
                        <li class="list-item">
                            [09/2019] I have joined the <a href="https://mmcheng.net/" target="_black">Media Computing Lab</a> under the supervision of Prof. <a href="https://mmcheng.net/cmm/" target="_black">Ming-Ming Cheng</a>!
                        </li>
                        <li class="list-item">
                            [06/2019] I graduated from Yangzhou University, and received my bachelor degree.
                        </li>
                        <li class="list-item">
                            [02/2018] One paper was accepted by ICASSP 2018.
                        </li>
                        <li class="list-item">
                            [07/2017] Two paper were accepted by ICONIP 2017, one of which is an oral paper.
                        </li>
                    </ul>
            </div>
        </section>

        <hr>



        <!-- Experiences -->
        <section>
            <h2>Experiences</h2>
            <div class="row">
                <div class="col-md-9">
                    <ul>
                        <li class="list-item">
                            <span class="title h4">Expert Researcher in Vision & Language</span> 
                            <div class="float-right h4">2023 - Now</div>
                            <div class="info h5 text-muted vspace-top-lite">Intelligent Creation</div>
                            <div class="info h5 text-muted">ByteDance</div>
                        </li>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./img/page_img/bytedance_logo_zh.png" class="img-responsive" height="58" width="228"/>
                </div>
            </div>
            <div class="row vspace-top">
                <div class="col-md-9">
                    <ul>
                        <li class="list-item">
                            <span class="title h4">Researcher in Vision & Language</span> 
                            <div class="float-right h4">2022 - 2023</div>
                            <div class="info h5 text-muted vspace-top-lite">Smart City Group (SCG)</div>
                            <div class="info h5 text-muted">SenseTime</div>
                        </li>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./img/page_img/sensetime_logo.png" class="img-responsive" height="65" width="228"/>
                </div>
            </div>
            <div class="row vspace-top">
                <div class="col-md-9">
                    <ul>
                        <li class="list-item">
                            <span class="title h4">Internship in Computer Vision</span> 
                            <div class="float-right h4">2020 - 2021</div>
                            <div class="info h5 text-muted vspace-top-lite">Youtu Lab</div>
                            <div class="info h5 text-muted">CSIG, Tencent</div>
                        </li>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./img/page_img/youtulab_logo.jpeg" class="img-responsive" height="69" width="228"/>
                </div>
            </div>
            <div class="row vspace-top">
                <div class="col-md-9">
                    <ul>
                        <li class="list-item">
                            <span class="title h4">M.S. in Computer Science</span> 
                            <div class="float-right h4">2019 - 2022</div>
                            <div class="info h5 text-muted vspace-top-lite">Media Computing Lab (supervised by Prof
                                <a href="https://mmcheng.net/cmm/" style="color:grey">Ming-Ming Cheng</a>)</div>
                            <div class="info h5 text-muted">School of Computer Science, Nankai University</div>
                        </li>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./img/page_img/nankai_logo.png" class="img-responsive" height="83" width="235"/>
                </div>
            </div>
            <div class="row vspace-top">
                <div class="col-md-9">
                    <ul>
                        <li class="list-item">
                            <span class="title h4">B.S. in Computer Science</span> 
                            <div class="float-right h4">2015 - 2019</div>
                            <div class="info h5 text-muted vspace-top-lite">College of Innovation and Entrepreneurship (Elite College)</div>
                            <div class="info h5 text-muted">School of Information Engineerin, Yangzhou University</div>
                        </li>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./img/page_img/yangda_logo.png" class="img-responsive" height="80" width="240"/>
                </div>
        </div>
        </section>

        <hr>

        <section>
            <h2>Publications</h2>
            <!-- 25 CreatiPoster -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <img src="./papers/25_creatiposter/creatiposter_logo.png" alt="logo" style="height: 22px; width: auto; vertical-align: middle;" />
                            <span class="paper-title">CreatiPoster: Towards Editable and Controllable <br> Multi-Layer Graphic Design Generation
                            </span>
                            <br />
                            <span class="paper-authors">
                                <strong class="text-info">Zhao Zhang</strong>, Yutao Cheng, Dexiang Hong, Maoke Yang <br> Gonglei Shi, Lei Ma, Hui Zhang, Jie Shao, Xinglong Wu
                            </span>
                            <br />
                            <span class="paper-pub">arXiv 2025 </span>  &nbsp
                            <a href="https://github.com/graphic-design-ai/creatiposter" target="_black">[Repo]</a>
                            <a href="https://arxiv.org/abs/2506.10890" target="_black">[Paper]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/25_creatiposter/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>


            <!-- 25 CreatiDesign -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">CreatiDesign: A Unified Multi-Conditional Diffusion Transformer <br> for Creative Graphic Design
                            </span>
                            <br />
                            <span class="paper-authors">
                                Hui Zhang, Dexiang Hong, Maoke Yang, Yutao Cheng, <strong class="text-info">Zhao Zhang</strong> <br> Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang
                            </span>
                            <br />
                            <span class="paper-pub">arXiv 2025 </span>  &nbsp
                            <a href="https://github.com/HuiZhang0812/CreatiDesign?tab=readme-ov-file" target="_black">[Repo]</a>
                            <a href="https://huizhang0812.github.io/CreatiDesign/" target="_black">[Project page]</a>
                            <a href="https://arxiv.org/pdf/2505.19114" target="_black">[Paper]</a>
                            <a href="./papers/25_creatidesign/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/25_creatidesign/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>


            <!-- 25 DeAM -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Decomposition of Graphic Design with Unified Multimodal Model
                            </span>
                            <br />
                            <span class="paper-authors">
                                Hui Nie, <strong class="text-info">Zhao Zhang</strong>, Yutao Cheng, Maoke Yang, Gonglei Shi, Qingsong Xie, Jie Shao, Xinglong Wu
                            </span>
                            <br />
                            <span class="paper-pub">ICML 2025</span>  &nbsp
                            <a href="#" target="_black">[Repo Coming Soon]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/25_deam/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 25 Layton -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Layton: Latent Consistency Tokenizer <br> for 1024-pixel Image Reconstruction and Generation by 256 Tokens
                            </span>
                            <br />
                            <span class="paper-authors">
                                Qingsong Xie, <strong class="text-info">Zhao Zhang</strong>, Zhe Huang, Yanhao Zhang, Haonan Lu, Zhenyu Yang
                            </span>
                            <br />
                            <span class="paper-pub">arXiv 2025</span>  &nbsp
                            <a href="https://arxiv.org/pdf/2503.08377" target="_black">[PDF]</a>
                            <a href="https://github.com/OPPO-Mente-Lab/Layton">[Project]</a>
                            <a href="./papers/25_layton/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/25_layton/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>
            <!-- 25 RelationLLM -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">RelationLMM: Large Multimodal Model <br> as Open and Versatile Visual Relationship Generalist
                            </span>
                            <br />
                            <span class="paper-authors">
                                Chi Xie, Shuang Liang, Jie Li, <strong class="text-info">Zhao Zhang</strong>, Feng Zhu, Rui Zhao
                            </span>
                            <br />
                            <span class="paper-pub">TPAMI 2025</span>  &nbsp
                            <a href="https://ieeexplore.ieee.org/abstract/document/10845195" target="_black">[Paper]</a>
                            <a href="./papers/25_relationlmm/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/25_relationlmm/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>
            <!-- 24 Graphist -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <img src="./papers/24_graphist/logo.png" alt="logo" style="height: 22px; width: auto; vertical-align: middle;" />
                            <span class="paper-title">Graphic Design with Large Multimodal Model
                            </span>
                            <br />
                            <span class="paper-authors">
                                Yutao Cheng*, <strong class="text-info">Zhao Zhang*</strong>, Maoke Yang*, Hui Nie, Chunyuan Li, Xinglong Wu, Jie Shao
                            </span>
                            <br />
                            <span class="paper-pub">AAAI 2025</span>  &nbsp
                            <a href="https://arxiv.org/abs/2404.14368" target="_black">[PDF]</a>
                            <a href="https://github.com/graphic-design-ai/graphist">[Project]</a>
                            <a href="./papers/24_graphist/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/24_graphist/keyimg.jpg" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 23 ISEKAI -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Link-Context Learning for Multimodal LLMs
                            </span>
                            <br />
                            <span class="paper-authors">
                                Yan Tai, Weichen Fan, <strong class="text-info">Zhao Zhang</strong>, Ziwei Liu
                            </span>
                            <br />
                            <span class="paper-pub">CVPR 2024</span>  &nbsp
                            <a href="https://arxiv.org/abs/2308.07891" target="_black">[PDF]</a>
                            <a href="https://github.com/isekai-portal/Link-Context-Learning"_black">[Code]</a>
                            <a href="./papers/23_isekai/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/23_isekai/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>
            
            <!-- 23 Shikra -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <img src="./papers/23_shikra/logo.png" alt="logo" style="height: 22px; width: auto; vertical-align: middle;" />
                            <span class="paper-title">Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic
                            </span>
                            <br />
                            <span class="paper-authors">
                                Keqin Chen, <strong class="text-info">Zhao Zhang*</strong>, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao
                            </span>
                            <br />
                            <span class="paper-pub">arXiv 2023</span>  &nbsp
                            <a href="https://arxiv.org/abs/2306.15195" target="_black">[PDF]</a>
                            <a href="https://github.com/shikras/shikra"_black">[Code]</a>
                            <a href="./papers/23_shikra/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/23_shikra/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 23 D-Cube -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Described Object Detection: Liberating Object Detection with Flexible Expressions
                            </span>
                            <br />
                            <span class="paper-authors">
                                Chi Xie*, <strong class="text-info">Zhao Zhang*</strong>, Yixuan Wu, Feng Zhu, Rui Zhao, Shuang Liang
                            </span>
                            <br />
                            <span class="paper-pub">NeurlPS 2023</span>  &nbsp
                            <a href="https://arxiv.org/abs/2307.12813" target="_black">[PDF]</a>
                            <a href="https://github.com/shikras/d-cube"_black">[Code]</a>
                            <a href="papers/23_dcube/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/23_dcube/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 23 GRES -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Advancing Referring Expression Segmentation Beyond Single Image
                            </span>
                            <br />
                            <span class="paper-authors">
                                Yixuan Wu*, <strong class="text-info">Zhao Zhang*</strong>, Chi Xie, Feng Zhu, Rui Zhao
                            </span>
                            <br />
                            <span class="paper-pub">ICCV 2023</span>  &nbsp
                            <a href="https://arxiv.org/abs/2305.12452" target="_black">[PDF]</a>
                            <a href="https://github.com/shikras/d-cube"_black">[Code]</a>
                            <a href="papers/23_GRES/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/23_GRES/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 25 IST -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">User-Oriented Interactive Style Transfer</span>
                            <br />
                            <span class="paper-authors">
                                Zheng Lin, <strong class="text-info">Zhao Zhang</strong>, Kang-Rui Zhang, Bo Ren, Ming-Ming Cheng
                            </span>
                            <br />
                            <span class="paper-pub">CVMJ 2025</span>  &nbsp
                            <a href="https://arxiv.org/abs/2203.13470" target="_black">[PDF]</a>
                            <a href="#"_black">[Code]</a>
                            <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                            <a href="./papers/22_IST/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_IST/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>
            <!-- 22 IHRR -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Image Harmonization by Matching Regional References</span>
                            <br />
                            <span class="paper-authors">
                                Ziyue Zhu, <strong class="text-info">Zhao Zhang</strong>, Zheng Lin, Ruiqi Wu, Chunle Guo
                            </span>
                            <br />
                            <span class="paper-pub">arXiv</span>  &nbsp
                            <a href="https://arxiv.org/abs/2204.04715" target="_black">[PDF]</a>
                            <a href="#"_black">[Code]</a>
                            <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                            <a href="./papers/22_IHRR/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_IHRR/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 23 PAMI -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Co-Salient Object Detection with Co-Representation Purification</span>
                            <br />
                            <span class="paper-authors">
                                Ziyue Zhu*, <strong class="text-info">Zhao Zhang*</strong>, Zheng Lin, Xing Sun, Ming-Ming Cheng
                            </span>
                            <br />
                            <span class="paper-pub">TPAMI 2023 </span>  &nbsp
                                <a href="#" target="https://ieeexplore.ieee.org/document/10008072">[PDF]</a>
                                <a href="#"_black">[Code]</a>
                                <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                                <a href="./papers/23_CoRP/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/23_CoRP/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 22 ECCV PAC-Net -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">PAC-Net: Highlight Your Video via History Preference Modeling</span>
                            <br />
                            <span class="paper-authors">
                                Hang Wang, Penghao Zhou, Chong Zhou, <strong class="text-info">Zhao Zhang</strong>, Xing Sun
                            </span>
                            <br />
                            <span class="paper-pub">ECCV 2022 </span>  &nbsp
                            <a href="#" target="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940602.pdf">[PDF]</a>
                            <a href="./papers/22_FocusCut/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_PACNet/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 22 ACM MM MMIIS -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Multi-Mode Interactive Image Segmentation</span>
                            <br />
                            <span class="paper-authors">
                                Zheng Lin, <strong class="text-info">Zhao Zhang</strong>*, Ling-Hao Han, Shao-Ping Lu
                            </span>
                            <br />
                            <span class="paper-pub">ACM MM 2022 </span>  &nbsp
                                <a href="#" target="_black">[PDF]</a>
                                <a href="#"_black">[Code]</a>
                                <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_MMIIS/keyimg.png" class="img-responsive" height="120" width="200"/>
                </div>
            </div>
            <!-- 22 ACM MM KnifeCut -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">KnifeCut: Refining Thin Part Segmentation with Cutting Lines</span>
                            <br />
                            <span class="paper-authors">
                                Zheng Lin, Zheng-Peng Duan, <strong class="text-info">Zhao Zhang</strong>, Chunle Guo, Ming-Ming Cheng
                            </span>
                            <br />
                            <span class="paper-pub">ACM MM 2022  (<b style="color:#ec4d4dc4";>Oral</b>) </span>  &nbsp
                                <a href="#" target="_black">[PDF]</a>
                                <a href="#"_black">[Code]</a>
                                <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_KnifeCut/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>
            <!-- 22 CVMJ SeqCut -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Sequential Interactive Image Segmentation</span>
                            <br />
                            <span class="paper-authors">
                                Zheng Lin, <strong class="text-info">Zhao Zhang</strong>, Zi-Yue Zhu, Deng-Ping Fan, Xia-Lei Liu
                            </span>
                            <br />
                            <span class="paper-pub">CVMJ 2022 </span>  &nbsp
                                <a href="#" target="_black">[PDF]</a>
                                <a href="#"_black">[Code]</a>
                                <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_SeqCut/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>
            <!-- 22 CVPR FocusCut -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">FocusCut: Diving into a Focus View in Interactive Segmentation</span>
                            <br />
                            <span class="paper-authors">
                                Zheng Lin, Zheng-Peng Duan, <strong class="text-info">Zhao Zhang</strong>, Chun-Le Guo, Ming-Ming Cheng
                            </span>
                            <br />
                            <span class="paper-pub">CVPR 2022 (<b style="color:#ec4d4dc4";>Oral</b>)</span>  &nbsp
                            <a href="#" target="_black">[PDF]</a>
                            <a href="#"_black">[Code]</a>
                            <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                            <a href="./papers/22_FocusCut/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/22_FocusCut/keyimg.jpg" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 20 ECCV GICD -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Gradient-Induced Co-Saliency Detection</span>
                            <br />
                            <span class="paper-authors">
                                <strong class="text-info">Zhao Zhang*</strong>, Wenda Jin*, Jun Xu, Ming-Ming Cheng
                            </span>
                            <br />
                            <span class="paper-pub">ECCV 2020</span>  &nbsp
                            <a href="https://arxiv.org/abs/2004.13364" target="_black">[PDF]</a>
                            <a href="./coca.html" target="_black">[Project]</a>
                            <a href="https://github.com/zzhanghub/gicd" target="_black">[Code]</a>
                            <a href="https://www.bilibili.com/video/BV1y5411a7Rq/" target="_black">[Short Video]</a>
                            <a href="https://www.bilibili.com/video/BV1bi4y137c6" target="_black">[Long Video]</a>
                            <a href="./papers/20_GICD/slides.pdf" target="_black">[Slides]</a>
                            <!-- <a href="./papers/20_GICD/translation.pdf" target="_black">[‰∏≠ËØëÁâà]</a> -->
                            <a href="#" target="_black">[‰∏≠ËØëÁâà]</a>
                            <a href="./papers/20_GICD/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/20_GICD/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 21 TIP BiANet -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Bilateral Attention Network for RGB-D Salient Object Detection</span>
                            <br />
                            <span class="paper-authors">
                                <strong class="text-info">Zhao Zhang</strong>, Zheng Lin, Jun Xu, Wenda Jin, Shao-Ping Lu, and Deng-Ping Fan
                            </span>
                            <br />
                            <span class="paper-pub">TIP 2021</span>  &nbsp
                            <a href="https://ieeexplore.ieee.org/document/9321705" target="_black">[PDF]</a>
                            <a href="https://github.com/zzhanghub/bianet" target="_black">[Code]</a>
                            <a href="./papers/20_BiANet/bibtex.txt" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/20_BiANet/keyimg.png" class="img-responsive img-thumbnail" height="60" width="200"/>
                </div>
            </div>

            <!-- 20 TNNLS D3Net -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Rethinking RGB-D Salient Object Detection: Models, Datasets, and Large-Scale Benchmarks</span>
                            <br />
                            <span class="paper-authors">
                                Deng-Ping Fan, Zheng Lin, <strong class="text-info">Zhao Zhang</strong>, Menglong Zhu, Ming-Ming Cheng
                            </span>
                            <br />
                            <span class="paper-pub">TNNLS 2020</span>  &nbsp
                            <a href="https://ieeexplore.ieee.org/document/9107477" target="_black">[PDF]</a>
                            <a href="https://github.com/DengPingFan/D3NetBenchmark" target="_black">[Code]</a>
                            <a href="http://dpfan.net/d3netbenchmark/" target="_black">[Project]</a>
                            <a href="#" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/20_D3Net/keyimg.png" class="img-responsive img-thumbnail" height="90" width="200"/>
                </div>
            </div>

            <!-- 20 CVPR FCA-Net -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Interactive Image Segmentation with First Click Attention</span>
                            <br />
                            <span class="paper-authors">
                                Zheng Lin, <strong class="text-info">Zhao Zhang</strong>, Lin-Zhuo Chen, Ming-Ming Cheng, Shao-Ping Lu
                            </span>
                            <br />
                            <span class="paper-pub">CVPR 2020</span>  &nbsp
                            <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Lin_Interactive_Image_Segmentation_With_First_Click_Attention_CVPR_2020_paper.html" target="_black">[PDF]</a>
                            <a href="https://github.com/frazerlin/fcanet" target="_black">[Code]</a>
                            <a href="https://mmcheng.net/fclick/" target="_black">[Project]</a>
                            <a href="#" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/20_FCANet/keyimg.png" class="img-responsive img-thumbnail" height="90" width="200"/>
                </div>
            </div>

            <!-- 18 ICASSP SRDCCA -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Low Resolution Face Recognition and Reconstruction <br> via Deep Canonical
                        Correlation Analysis</span>
                            <br />
                            <span class="paper-authors">
                                <strong class="text-info">Zhao Zhang</strong> Yun-Hao Yuan, Xiao-bo Shen, Yun Li
                            </span>
                            <br />
                            <span class="paper-pub">ICASSP 2018</span>  &nbsp
                            <a href="https://ieeexplore.ieee.org/abstract/document/8461985/" target="_black">[PDF]</a>
                            <a href="#" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/18_SRDCCA/keyimg.png" class="img-responsive img-thumbnail" height="90" width="200"/>
                </div>
            </div>

            <!-- 17 ICONIP SRKCCA -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Face Hallucination and Recognition Using Kernel Canonical Correlation
                                Analysis</span>
                            <br />
                            <span class="paper-authors">
                                <strong class="text-info">Zhao Zhang</strong> Yun-Hao Yuan, Yun Li, Bin Li, Ji-Peng Qiang
                            </span>
                            <br />
                            <span class="paper-pub">ICONIP 2017 (<b style="color:#ec4d4dc4";>Oral</b>)</span>)</span>  &nbsp
                            <a href="https://link.springer.com/chapter/10.1007/978-3-319-70136-3_67" target="_black">[PDF]</a>
                            <a href="#" target="_black">[Slides]</a>
                            <a href="#" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
                <div class="col-md-3">
                    <img src="./papers/17_SRKCCA/keyimg.png" class="img-responsive img-thumbnail" height="90" width="200"/>
                </div>
            </div>

            <!-- 17 ICONIP SDCCA -->
            <div class="row vspace-top-lite">
                <div class="col-md-9">
                    <ul>
                        <p>
                            <span class="paper-title">Supervised Deep Canonical Correlation Analysis for Multiview Feature
                                Learning</span>
                            <br />
                            <span class="paper-authors">
                                Yan Liu, Yun Li, Yun-Hao Yuan, Ji-Peng Qiang, Min Ruan,
                                <strong class="text-info">Zhao Zhang</strong>
                            </span>
                            <br />
                            <span class="paper-pub">ICONIP 2017</span>  &nbsp
                            <a href="https://link.springer.com/chapter/10.1007/978-3-319-70136-3_61" target="_black">[PDF]</a>
                            <a href="#" target="_black">[bib]</a>
                        </p>
                    </ul>
                </div>
            </div>

        </section>
        <hr>

    </div>


    <div class="container">
        <!-- News -->
        <section class="row vspace-top">
            <div class="col-lg-10">
                <h2>Services</h2>
                    <ul>
                        <li class="list-item">
                            Reviewer for T-PAMI, TIP, TMI, TMM, TCSVT, CVPR, ICCV, ECCV, NeurIPS, EMNLP, ACMMM, etc.
                        </li>
                    </ul>
            </div>
        </section>

        <hr>





    </div>




    <footer class="py-3 bg-dark vspace-top">
        <div class="container">
            <a href="https://beian.miit.gov.cn" target="_blank">
                <p class="m-0 text-center text-white">ËãèICPÂ§á16068582Âè∑-3</p>
            </a>
        </div>
    </footer>


    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <!-- <script src="https://kit.fontawesome.com/643e792d0b.js" crossorigin="anonymous"></script> -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
</body>

</html>